{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "a24045ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee0f9e1c884b4574993a9ef93ef27750",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='choix:', options=('---', 'saisir fonction', 'fonction de test'), value='---')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "eb6f0ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Widget text has value fonction de test,\n",
      "Widget text has value My Text,\n"
     ]
    }
   ],
   "source": [
    "print('Widget text has value {},'.format(menu.value))\n",
    "print('Widget text has value {},'.format(text.value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "7b83e614",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec22836fefe1472e9464e6a0f59646cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='choix:', options=('---', 'fct1', 'fct2', 'fct3', 'fct4'), value='---')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf0fb967",
   "metadata": {},
   "outputs": [],
   "source": [
    "#group 1\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#utilisez intervalle  dans R2 [ax,bx][ay,by]\n",
    "def GrapheLN(f,ax,bx,ay,by,N,c):\n",
    "    # Si vous voulez utiliser sin,cos,exp etc ... utlisez numpy exemple : np.sin(x+y)\n",
    "#     f=eval('lambda x,y: ' + input(\"Entrer fonction:\"))\n",
    "    if (c==0):\n",
    "        f = eval('lambda x: ' + input('Entrer fonction:'))\n",
    "        lx = [ax+i*(bx-ax)/N for i in range(N+1)]\n",
    "        ly = [f(x) for x in lx]\n",
    "        plt.figure(figsize=(10,12))\n",
    "        plt.subplot(1,2,1)\n",
    "        plt.plot(lx,ly,\"b\", label=\"f(x)\")\n",
    "        plt.axhline(color = 'k')\n",
    "        plt.axvline(color='k')\n",
    "        plt.xlabel(\"Abscisses\")\n",
    "        plt.ylabel('Ordonnees')\n",
    "        plt.title(\"Tracé approché d'un graphe\")\n",
    "        plt.grid()\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    else:\n",
    "        X, Y = np.meshgrid(np.linspace(ax,bx,N), np.linspace(ay,by,N))\n",
    "        Z = f(X,Y)\n",
    "        plt.figure(1)\n",
    "        plt.title(\"Tracé approché du graphe\")\n",
    "        ax = plt.axes(projection='3d')\n",
    "        ax.plot_surface(X,Y,Z)\n",
    "        plt.figure(2)\n",
    "        graphe = plt.contour(X,Y,Z,N)\n",
    "        plt.clabel(graphe,inline=1,fontsize=5,fmt='%3.2f')\n",
    "        plt.title(\"Tracé approché des lignes de niveaux\")\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "# GrapheLN(-1,1,-1,2,20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "183d62ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#group 2\n",
    "import numpy\n",
    "import numpy as np\n",
    "import sympy\n",
    "from sympy import *\n",
    "\n",
    "# from sympy import symbols, Eq, solve, log\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "def partial(element, function): #decalaration du fonction partiel ( element x,y ; funcion )\n",
    "   \n",
    "    partial_diff = function.diff(element) #dérive de f par rapprt x et y \n",
    "\n",
    "    return partial_diff # resultat \n",
    "\n",
    "def determat(partials_second, cross_derivatives, singular, symbols_list): #decalaration de det pour determiner min et max \n",
    "\t\n",
    "\tdet = partials_second[0].subs([(symbols_list[0], singular[symbols_list[0]]), (symbols_list[1], singular[symbols_list[1]])]) * partials_second[1].subs([(symbols_list[0], singular[symbols_list[0]]), (symbols_list[1], singular[symbols_list[1]])]) - (cross_derivatives.subs([(symbols_list[0], singular[symbols_list[0]]), (symbols_list[1], singular[symbols_list[1]])]))**2\n",
    "\n",
    "\treturn det # resultat de determinant \n",
    "\n",
    "def gradient(partials): #declaration du la fonction gradient \n",
    "  \n",
    "    grad = numpy.matrix([[partials[0]], [partials[1]]]) # partials fonction du bib numpy , partial : dérive de f  par rapport ( x et y ) // 0 et 1 ordre de x et y\n",
    "\n",
    "    return grad #resultat du vecteur gradient\n",
    "\n",
    "\n",
    "def gradient_to_zero(symbols_list, partials): #systeme d'equation = 0 // determiner point critique \n",
    "    \n",
    "    partial_x = Eq(partials[0], 0) # partial par rapport x = 0\n",
    "    partial_y = Eq(partials[1], 0) # partial par rapport y = 0\n",
    "\n",
    "    singular = solve((partial_x, partial_y), (symbols_list[0], symbols_list[1])) # solve : resolution du system , symobols_list : les point critique en x et en y \n",
    "\n",
    "    return singular # resultat des point critiques \n",
    "\n",
    "\n",
    "def hessian(partials_second, cross_derivatives): # declaration du matrice hessien (dérivé second de f (x,y) par rapport (x/y) ; dérivé second de (f(x)/x) et (f(y)/y))\n",
    "   \n",
    "    hessianmat = numpy.matrix([[partials_second[0], cross_derivatives], [cross_derivatives, partials_second[1]]]) # forme du matrice \n",
    "\n",
    "    return hessianmat #resultat du matrice hessein \n",
    "\n",
    "\n",
    "def fonction2(test):\n",
    "    if(test==1):\n",
    "        x, y = symbols('x y') #les elements \n",
    "        symbols_list = [x, y] \n",
    "        function = (x-1)**2+(y-4)**2\n",
    "    elif(test==2):\n",
    "        x, y = symbols('x y') #les elements \n",
    "        symbols_list = [x, y] \n",
    "        function = x**2+y**2\n",
    "    elif(test==3):\n",
    "        x, y = symbols('x y') #les elements \n",
    "        symbols_list = [x, y]\n",
    "        function = (1-x)**2+100*(y-x**2)**2\n",
    "\n",
    "    partials, partials_second = [],[] # dérive  du f par rapport x et y  // dérive seccond de f  par rapport x et y \n",
    "\n",
    "    for element in symbols_list:\n",
    "        partial_diff = partial(element, function) #appel de system du fonction dérive partiel \n",
    "        partials.append(partial_diff) # append : ajouter des éléments à une liste \n",
    "\n",
    "    grad = gradient(partials) # appel de fonction au vecteur gradient\n",
    "    singular = gradient_to_zero(symbols_list, partials) #appel de fonction au fonction singular : pour les point critiques \n",
    "\n",
    "    cross_derivatives = partial(symbols_list[0], partials[1]) #appel de fonction pour dérivé second \n",
    "\n",
    "    for i in range(0, len(symbols_list)): #nombre de point critique possible selon la fonction \n",
    "        partial_diff = partial(symbols_list[i], partials[i]) # i = nombre des point critiques // \n",
    "        partials_second.append(partial_diff) # ajouter des éléments de f à la liste \n",
    "\n",
    "    hessianmat = hessian(partials_second, cross_derivatives) #appel au fonction hessien \n",
    "    det = determat(partials_second, cross_derivatives, singular, symbols_list) # calculer derterminnats du matrices hessien\n",
    "    \n",
    "    print(\"Vecteur Gradient du fonction : {0} est :\\n {1}\".format(function,grad)) # affichage de fontion + Vecteur gradient\n",
    "    print(\"Déterminant au point critique {0} est :\\n {1}\".format(singular, det)) # afficher du point critiques  \n",
    "    print(\"Matrice hessienne qui organise toutes les dérivées partielles secondes de la fonction {0} est :\\n {1}\".format(\n",
    "        function,hessianmat)) # affichage de fontion + marice hessien \n",
    "     \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e220184",
   "metadata": {},
   "outputs": [],
   "source": [
    "#group 3\n",
    "import numpy\n",
    "\n",
    "#Definition de la fonction 2\n",
    "def fonction3(test):\n",
    "    if (test==2):\n",
    "        def fct2(x):\n",
    "            y = numpy.asarray(x)\n",
    "            return numpy.sum(y[0]**2+y[1]**4)\n",
    "\n",
    "        #Calcul du gradient de la fonction 2\n",
    "        def fct2Gradient(x):\n",
    "            y = numpy.asarray(x)\n",
    "            grad = numpy.zeros_like(y)\n",
    "            grad[0] = 2*y[0]\n",
    "            grad[1] = 4*(y[1]**3)\n",
    "            return grad\n",
    "    elif(test==3):\n",
    "        def fct2(x):\n",
    "            y = numpy.asarray(x)\n",
    "            return numpy.sum((y[0]-1)**2+100*(y[1]-y[0]**2)**2)\n",
    "\n",
    "        #Calcul du gradient de la fonction de Rosenbrock\n",
    "        def fct2Gradient(x):\n",
    "            y = numpy.asarray(x)\n",
    "            grad = numpy.zeros_like(y)\n",
    "            grad[0] = 400*y[0]*(y[0]**2-y[1])+2*(y[0]-1)\n",
    "            grad[1] = 200*(y[1]-y[0]**2)\n",
    "            return grad\n",
    "    x0 = numpy.array([1.1,2.1]) \n",
    "    point_error = 10**-5\n",
    "    gradient_error = 10**-5\n",
    "\n",
    "\n",
    "    #Appel de la fonction de calcul du gradient a pas fixe.\n",
    "    Gradient_Pas_Fixe(fct2,fct2Gradient,gradient_error,point_error,x0,10**-3,10000)\n",
    "\n",
    "#Definition de la fonction pour le calcul du gradient a pas fixe.\n",
    "def Gradient_Pas_Fixe(f,f_grad,gradient_error,point_error,x0,Tolerance,NB_ITR):\n",
    "    dimension = numpy.max(numpy.shape(x0))\n",
    "    XArray = numpy.zeros([dimension,NB_ITR])\n",
    "    fArray = numpy.zeros(NB_ITR)\n",
    "    point_error_array = numpy.zeros(NB_ITR)\n",
    "    gradient_error_array = numpy.zeros(NB_ITR)\n",
    "    x = numpy.asarray(x0)\n",
    "    xx = x\n",
    "    grad = f_grad(x)\n",
    "    for i in range(NB_ITR):\n",
    "        x = x - Tolerance*f_grad(x)\n",
    "        grad_x = f_grad(x)\n",
    "        ff = f(x)\n",
    "        XArray[:,i] = x\n",
    "        fArray[i] = ff\n",
    "        point_error_array[i] = numpy.linalg.norm(x - xx)\n",
    "        gradient_error_array[i] = numpy.linalg.norm(grad)\n",
    "#         if i % 100 == 0: #Affichage des rèsultas chaque 100 itération\n",
    "#             print(f\"Iteration={i+1}, x={x}, f(x)={f(x)}\")\n",
    "        if (point_error_array[i]<point_error)|(gradient_error_array[i]<gradient_error):\n",
    "            break\n",
    "        xx = x\n",
    "    #Affichage du résultat final\n",
    "    print(\"---------------------------------------------------------\")\n",
    "    print(\"Final Results:\\n\")\n",
    "    print(f\"x={x}\\nIteration={i+1}\\nf(x)={f(x)}\")\n",
    "    \n",
    "    return {'XArray':XArray[:,0:i],'fArray':fArray[0:i],'point_error_array':point_error_array[0:i],'point_error_array':point_error_array[0:i]}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59a51e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#group 4\n",
    "# import sympy\n",
    "# from sympy import *\n",
    "\n",
    "#----------------------------------------------------------------------------------------#\n",
    "# Gradient Descent Implementation\n",
    "class GradientDescent_RN():\n",
    "    def __init__(self, l_rate=0.01, max_iter=1000, precision=1e-5, function=None,variables=None):\n",
    "        self.l_rate = l_rate  # learning rate\n",
    "        self.max_iter = max_iter  # Nb max d'iteration\n",
    "        self.precision = precision  # stop prev_step_sizeition = precision\n",
    "        self.function = function # function \n",
    "        self.variables = variables # symbols x,y,z ...\n",
    "\n",
    "    # Input : Array of values : expl = (1,2,3,4)\n",
    "    # Input : symbol_deriv = exemple calculate partial deriv in x \n",
    "    def _partial_derivative(self, symbol_deriv, values):\n",
    "        if self.function and self.variables and values:\n",
    "            dif1 = diff(self.function,symbol_deriv)\n",
    "            new_list_tuple = []\n",
    "            i = 0\n",
    "            for ar in values:\n",
    "                new_list_tuple.append((self.variables[i],ar))\n",
    "                i+=1\n",
    "            res = dif1.subs(new_list_tuple)\n",
    "        del new_list_tuple\n",
    "        return res\n",
    "\n",
    "    def _calculate_fct(self, values):\n",
    "        if self.function and self.variables and values:\n",
    "            new_list_tuple = []\n",
    "            i = 0\n",
    "            for ar in values:\n",
    "                new_list_tuple.append((self.variables[i],ar))\n",
    "                i+=1\n",
    "        return self.function.subs(new_list_tuple)\n",
    "\n",
    "    def start_gradient(self):\n",
    "        n_dimension = len(self.variables)\n",
    "        listofvars = [1.1] * n_dimension\n",
    "        res = self._calculate_fct(listofvars)  # calculate f(x1,x2,..) = res\n",
    "        prev_step_size = 1  # start with prev_step_size greater than eps (assumption)\n",
    "        nb_iter = 0  # init iteration counter\n",
    "        prev_res = res  # init previous res\n",
    "        while prev_step_size > self.precision and nb_iter < self.max_iter:\n",
    "            for i in range(n_dimension):\n",
    "                listofvars[i] = listofvars[i] - self.l_rate * self._partial_derivative(self.variables[i],listofvars)  # Make a small step down Xn\n",
    "            res = self._calculate_fct(listofvars)\n",
    "            nb_iter = nb_iter + 1  # iteration count\n",
    "            prev_step_size = abs(prev_res - res)  # Change in res\n",
    "            prev_res = res  # Store current res value in prev_res\n",
    "        if nb_iter == self.max_iter:\n",
    "            print(\"Maximum Iterations Exceeded : Could not find local Mininmun.\")\n",
    "            return\n",
    "        print(\"Iterations : \", nb_iter)\n",
    "        new_list = [str(self.variables[idx]) + \" = \"+ str(round(val,2)) for idx, val in enumerate(listofvars)]\n",
    "        print(\"Found Local Minimum = {}; in points {}\".format(res,new_list))\n",
    "        \n",
    "    \n",
    "# Function fct2\n",
    "def fonction4(test):\n",
    "    if(test==2):\n",
    "        def fct2():\n",
    "            x, y = symbols('x y')\n",
    "            fct2_exp = x**2 + y**4\n",
    "            return sympy.expand(fct2_exp), x, y\n",
    "\n",
    "    # Rosenbrock Function\n",
    "    elif(test==3):\n",
    "        def fct2():\n",
    "            x, y = symbols('x y')\n",
    "            fct2_exp = (1-x)**2 + 100 * (y-x**2)**2\n",
    "            return sympy.expand(fct2_exp), x, y\n",
    "    elif(test==1):\n",
    "        def fct2():\n",
    "            x, y = symbols('x y')\n",
    "            fct2_exp = (x-1)**2 + (y-4)**2\n",
    "            return sympy.expand(fct2_exp), x, y\n",
    "    fn = fct2()\n",
    "    gradient_fct2 = GradientDescent_RN(function=fn[0],variables=fn[1::])\n",
    "    gradient_fct2.start_gradient()\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f2ce398",
   "metadata": {},
   "outputs": [],
   "source": [
    "#group 5\n",
    "import string\n",
    "# from sympy import *\n",
    "import sys\n",
    "def initForGrad(pexp, ppoint):\n",
    "    \"\"\"\n",
    "    Cette fonction permet d'initialiser des variables necessaires au differente fonction de gradient\n",
    "    le paramètre pexp est l'expression de la fonction\n",
    "    le paramètre ppoint: premier point X0\n",
    "    le retour: liste des variable de pexp, nombre de variable, symbole p du pas, initialisation d'un gradient correspondant à pexp, [[(var1, val1), ...,(varn, valn)]\n",
    "    \"\"\"\n",
    "    variables = pexp.free_symbols\n",
    "    return variables, len(variables), Symbol('p'), [pexp.diff(var) for var in variables], list(zip(variables, ppoint))\n",
    "def Xk(pvec, ppas, pgrad, pdim, pmod=0, pcond=1):\n",
    "    \"\"\"\n",
    "    cette fonction renvoie le point X k+1 pour une regression\n",
    "    le paramètre pvec: [(var1, val1), ...,(varn, valn)] tableau qui associe au valeur de ppnt le nom de la variable à laquel\n",
    "   le paramètre ppas: indique le pas necessaire pour l'iteration suivante\n",
    "    le paramètre pgrad: [derivepartielEnVar1, .. , derivepartielEnVarn] gradient de la fonction d'origine\n",
    "   le paramètre pdim: n dimension du vecteur de la fonction\n",
    "    le retour: [val1, .. , valn] point à l'iteration k+1 soit Xk+1\n",
    "    \"\"\"\n",
    "    if pmod == 0:\n",
    "        res = [pvec[i][1] - ppas * pgrad[i].subs(pvec) for i in range(pdim)]\n",
    "    elif pmod == 2:\n",
    "        res = [pvec[i][1] + ppas * pgrad[i] for i in range(pdim)]\n",
    "    else:\n",
    "        res = [(pvec[i][1] - ppas * (pgrad[i].subs(pvec) / pcond)).evalf() for i in range(pdim)]\n",
    "    return res\n",
    "def expPas(ppnt, pgrad, pvec, pdim):\n",
    "    \"\"\"\n",
    "    cette fonction renvoie l'expression de phi en fonction de p soit un vecteur de même dimension que pvec. Cette expression\n",
    "    sert par la suite à calculer le pas optimal\n",
    "    le paramètre ppnt: [val1, .. , valn] point à partir duquel le pas optimal\n",
    "    le paramètre pgrad: [derivepartielEnVar1, .. , derivepartielEnVarn] gradient de la fonction d'origine\n",
    "    le paramètre pvec: [(var1, val1), ...,(varn, valn)] tableau qui associe au valeur de ppnt le nom de la variable à laquel\n",
    "    elle est associé\n",
    "    le paramètre pdim: n dimension du vecteur de la fonction\n",
    "    le retour: expression de phi de p\n",
    "    \"\"\"\n",
    "    return [parse_expr(str(ppnt[i]) + \" - p * \" + str(pgrad[i].subs(pvec))) for i in range(pdim)]\n",
    "def pasOpti(pexprpas, pvec, pp):\n",
    "    \"\"\"\n",
    "    cette fonction permettant de calculer le pas optimal pour Xk+1\n",
    "    le paramètre pexprpas: expression de f\n",
    "    le paramètre pvec: vecteur trouver pour phi de p\n",
    "    le paramètre pp: symbole du pas\n",
    "    le retour: est le pas optimisé sinon -1 si fonction a échoué\n",
    "    \"\"\"\n",
    "    pas = solve(pexprpas.subs(pvec), pp)  # fin calcul du pas opti\n",
    "    res = -1\n",
    "    for e in pas:\n",
    "        if e > 0:\n",
    "            res = e\n",
    "            break\n",
    "    return res\n",
    "def gradPOpti(p_exp, ppt, tolerance):\n",
    "    \"\"\"\n",
    "    cette fonction effectue une descente de gradient a pas optimisé\n",
    "    le paramètre p_exp: expression de la fonction sur laquel effectuer une descente de gradient\n",
    "    le paramètre ppt: [val1, .. , valn] point de depart de la descente de gradient\n",
    "    le paramètre tolerance: de type float est le seuil à partir duquel on decidera que l'aproximation est suffisante (par rapport a la norme\n",
    "    du point trouvé\n",
    "    le paramètre pverbose: int par defaut à 0 et si different de 0 alors le mode verbose est activé\n",
    "    le retour: [val1, .. , valn] point au plus proche du minimum local\n",
    "    \"\"\"\n",
    "    variables, size, p, grad, vec = initForGrad(p_exp, ppt)\n",
    "    expas = expPas(ppt, grad, vec, size)\n",
    "    pas = pasOpti(p_exp, list(zip(variables, expas)), p)\n",
    "    XK1 = Xk(vec, pas, grad, size)\n",
    "    vec = list(zip(variables, XK1))\n",
    "    cond = Matrix([grad[i].subs(vec) for i in range(size)]).norm()\n",
    "    nombre_itter=1;\n",
    "    while cond > tolerance:\n",
    "        nombre_itter=nombre_itter+1\n",
    "        print(\"X{} : {}\".format( nombre_itter, XK1))\n",
    "        expas = expPas(ppt, grad, vec, size)\n",
    "        pas = pasOpti(p_exp, list(zip(variables, expas)), p)\n",
    "        if pas == -1:\n",
    "            break\n",
    "        XK1 = Xk(vec, pas, grad, size)\n",
    "        vec = list(zip(variables, XK1))\n",
    "        cond = Matrix([grad[i].subs(vec) for i in range(size)]).norm()\n",
    "    print (\"Pas Optimal\",pas)\n",
    "    print(\"nombre d'ittération\",nombre_itter)\n",
    "    return XK1\n",
    "\n",
    "def fonction5(test,nbbb):\n",
    "    f = parse_expr(\"x**2+y**2+z**2-3*y*z-6*x*y\")\n",
    "    b=0.0001\n",
    "    x0=[1,1,1]\n",
    "    if(test==1):\n",
    "        f = parse_expr(\"(x-1)**2+(y-4)**2\")\n",
    "    elif(test==2):\n",
    "        f = parse_expr(\"x**2+y**2\")\n",
    "    elif(test==3):\n",
    "        f = parse_expr(\"(1-x)**2+100*(y-x**2)**2\")\n",
    "    elif(test==4):\n",
    "        f = parse_expr(input(\"Entrez la fonction: \"))\n",
    "        x0=nbbb*[1]\n",
    "    res = gradPOpti(f, x0, float(b))\n",
    "    variables = f.free_symbols\n",
    "    vec = list(zip(variables, res))\n",
    "    print(\"point au plus proche du minimum local {}\".format(f.subs(vec)))\n",
    "    print(\"approximation trouvé : {}\".format(vec))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2634489f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#group 6\n",
    "## importations\n",
    "import random as r\n",
    "#import numpy as np\n",
    "import math as m\n",
    "import scipy.linalg as spl # in order to solve Ax=b\n",
    "#import matplotlib.pyplot as plt \n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# # Création d'une matrice définie positive (matrice SDP)\n",
    "    # n : taille de la matrice\n",
    "    # nb_extra_diag: nombre d'extra-diagonales non égal à zéro\n",
    "def matrice_SDP(n, nb_extra_diag):\n",
    "    A = np.zeros([n, n])\n",
    "    coord = []\n",
    "    for i in range(0, n, 1):\n",
    "        for j in range(0, i, 1):\n",
    "            coord.append([i, j])\n",
    "    for i in range(0, nb_extra_diag, 1):\n",
    "        c = r.randint(0, len(coord)-1)\n",
    "        val = r.randint(0,50)\n",
    "        A[coord[c][0]][coord[c][1]] = val\n",
    "        A[coord[c][1]][coord[c][0]] = val\n",
    "        del coord[c]\n",
    "    for i in range(0, n, 1):\n",
    "        s = 0\n",
    "        for j in range(0, n, 1):\n",
    "            s = s + A[i][j]\n",
    "        A[i][i] = r.randint(s + 1, 2 * s +1)\n",
    "    return A\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def matriceA_SDP(n, nb_extra_diag):\n",
    "    A = np.zeros([n, n])\n",
    "    \n",
    "    for i in range(0,n) :\n",
    "        \n",
    "        if i < n :\n",
    "            A[i - 1, i] = -1\n",
    "            A[(i-n) + 1, i] = -1\n",
    "            A[0,n-1] =0\n",
    "            A[n-1,0] =0\n",
    "    print(\"\\n Entrer les élements du diagonal :\\n \")\n",
    "    for i in range(0, nb_extra_diag, 1):\n",
    "        A[i][i]= int(input(\"\"))\n",
    "  \n",
    "    return A\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Création d'un vecteur aléatoire de taille n, avec une valeur comprise entre 0 et 50.\n",
    "def random_vector(n):\n",
    "    B = np.zeros((n,1))\n",
    "    for i in range(0,n,1):\n",
    "        B[i][0] = r.randint(0,50)\n",
    "    return B\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## vérifier si la matrice est définie positive\n",
    "def is_symdefpos(M):\n",
    "    # vérifier si M est symétrique : si AT = A\n",
    "    for i in range(np.shape(M)[0]):\n",
    "        for j in range(np.shape(M)[1]):\n",
    "            if (M[i][j] != M[j][i]):\n",
    "                return False\n",
    "\n",
    "    # vérifier si M est définie positif : si toutes ses valeurs propres sont strictement positives.\n",
    "    if not (  np.all(np.linalg.eigvals(M) > 0)):\n",
    "      return False\n",
    "\n",
    "    # vérifier si M est inversible : si det !=0\n",
    "    if spl.det(M) == 0:\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## gradient conjugé\n",
    "def conjgrad(A,b,X,imax,p):\n",
    "    if (is_symdefpos(A) == False):\n",
    "        # vérifier si A est SDP\n",
    "        print(\"\\n A n'est pas symétrique définie positive\")\n",
    "        return np.zeros((np.shape(A)[0],1))\n",
    "    R = b - A.dot(X)\n",
    "    P = R\n",
    "    rs_old = np.transpose(R).dot(R)\n",
    "    for i in range(1, imax + 1):\n",
    "        Ap = A.dot(P)\n",
    "        alpha = rs_old / np.transpose(P).dot(Ap)\n",
    "        X = X + (alpha * P)\n",
    "        R = R - (alpha * Ap)\n",
    "        rs_new = np.transpose(R).dot(R)\n",
    "        if (m.sqrt(rs_new) < p):\n",
    "            break\n",
    "        P = R + (rs_new/rs_old) * P\n",
    "        rs_old = rs_new\n",
    "    print(\"\\n R = \\n\", R)\n",
    "    print(\"\\n X = \\n\", X)\n",
    "    return X\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## fonction test\n",
    "def fonction6():\n",
    "        \n",
    "# initialiser la matrice de test\n",
    "    \n",
    "    \n",
    "    A = np.array([[3.,-1.,0.,0.,0.],[-1.,12.,-1.,0.,0.],[0.,-1.,24.,-1.,0.],[0.,0.,-1.,48.,-1.],[0.,0.,0.,-1.,96.]])\n",
    "    print(\"A= \\n\", A, \"\\n Dimension de A: \", np.shape(A))\n",
    "    b = np.array([[1.],[2.],[3.],[4.],[5.]])\n",
    "    print(\"B = \\n\", b, \"\\n Dimension de b: \", np.shape(b))\n",
    "    Xzero = np.array([[0.],[0.],[0.],[0.],[0.]]) # initialized null 5-dimensioned vector\n",
    "    imax = 10**3\n",
    "    p = 10**(-5)\n",
    "    print(\"\\n Précision à \",p,\" près \\n\")\n",
    "    X_sol = conjgrad(A, b, Xzero, imax, p)\n",
    "    print(\"\\n Avec la méthode du gradient conjugué on a \\n X = \\n\", X_sol)\n",
    "    \n",
    "    #on va vérifier notre solution\n",
    "    X_solve = spl.solve(A, b)\n",
    "    print(\"\\n Avec la fonction solve de la biliothèque scipy.linalg on a \\n X = \\n\", X_solve) \n",
    "    print(\"\\n Test avec une matrice générée par la fonction matrice_SDP : \\n\")\n",
    "\n",
    "    print(\" - Entrer 1 si vous voulez de saisir une fonction quadratique \\n - Entrer 2 si vous voulez une matrice aléatoire avec taille et nombre d'elements extra-diagonaux saisies (Default)\") \n",
    "    choix = int(input())\n",
    "    matrix_size = int(input(\"Entrer la taille de la matrice A: \"))\n",
    "    nbr_extra_diag = int(input(\"Ainsi que le nombre d'elements extra-diagonaux de A: \"))\n",
    "    if (choix==1):\n",
    "      A = matriceA_SDP(matrix_size, nbr_extra_diag)\n",
    "    else:\n",
    "      A = matrice_SDP(matrix_size, nbr_extra_diag)\n",
    "        \n",
    "    b = random_vector(matrix_size)\n",
    "    Xzero = np.zeros((matrix_size,1))\n",
    "    X_sol = conjgrad(A, b, Xzero, imax, p)\n",
    "\n",
    "     #on va vérifier notre solution\n",
    "    X_solve = spl.solve(A, b)\n",
    "    print(\"\\n Avec la fonction solve de la biliothèque scipy.linalg on a \\n X = \\n\", X_solve) \n",
    "\n",
    "    print(\"\\n A = \\n\", A)\n",
    "    print(\"\\n b = \\n\", b)\n",
    "    print(\"\\n Xzero = \\n\", Xzero)\n",
    "    print(\"\\n X_solve = \\n\", X_sol)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a7524f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#group 7\n",
    "# from time import time\n",
    "\n",
    "# import numpy as np\n",
    "# from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "def armijo(max_iters=200, A=None, b=None):\n",
    "    b = b if b is not None else np.matrix([[1], [2], [3], [4], [5]])\n",
    "    A = A if A is not None else np.matrix([[3, -1, 0, 0, 0],\n",
    "                                           [-1, 12, -1, 0, 0],\n",
    "                                           [0, -1, 24, -1, 0],\n",
    "                                           [0, 0, -1, 48, -1],\n",
    "                                           [0, 0, 0, -1, 96]])\n",
    "\n",
    "    n = len(b)\n",
    "\n",
    "    def f(x):\n",
    "        \"\"\"\n",
    "        calculate f(x) with input x\n",
    "        \"\"\"\n",
    "        return (1 / 2 * np.dot((np.dot(x.T, A)), x) - np.dot(b.T, x)).item()\n",
    "\n",
    "    def df(x):\n",
    "        \"\"\"\n",
    "        calculate f'(x) with input x\n",
    "        \"\"\"\n",
    "        return A * x + b\n",
    "\n",
    "    def grad_desc_exact():\n",
    "        \"\"\"\n",
    "        gradient descent with exact line minimization\n",
    "        \"\"\"\n",
    "        x = np.matrix(np.zeros((n, 1)))\n",
    "        fun_values_exact = [f(x)]\n",
    "        for i in range(max_iters - 1):\n",
    "            # calculate derivative\n",
    "            d = df(x)\n",
    "            # calculate step size\n",
    "            alpha = (d.T * d / (d.T * A * d)).item()\n",
    "            # update x\n",
    "            x -= alpha * d\n",
    "            # get new function value\n",
    "            fun_values_exact.append(f(x))\n",
    "            return fun_values_exact\n",
    "\n",
    "    def grad_desc_armijo(alpha, beta=0.5, sigma=0.9):\n",
    "        \"\"\"\n",
    "        gradient descent with Armijo step size rule\n",
    "        \"\"\"\n",
    "        fun_values_exact = grad_desc_exact()\n",
    "        x = np.matrix(np.zeros((n, 1)))\n",
    "        fun_values_armijo = [f(x)]\n",
    "        curr_iter = 1\n",
    "        for i in range(max_iters - 1):\n",
    "            # calculate derivative\n",
    "            d = df(x)\n",
    "            # backtracking line search\n",
    "            cur_alpha = alpha\n",
    "            cur_value = f(x + cur_alpha * d)\n",
    "            while cur_value > f(x) + sigma * cur_alpha * d.T * d:\n",
    "                cur_alpha *= beta\n",
    "                cur_value = f(x + cur_alpha * d)\n",
    "            # update x\n",
    "            x -= cur_alpha * d\n",
    "            # get new function value\n",
    "            fun_values_armijo.append(cur_value)\n",
    "            if cur_value == fun_values_armijo[-1]:\n",
    "                curr_iter = curr_iter\n",
    "            else:\n",
    "                curr_iter += 1\n",
    "        print('nombre d\\'iteration Armijo ', curr_iter)\n",
    "        return fun_values_armijo\n",
    "\n",
    "    results = grad_desc_armijo(alpha=1)\n",
    "    plt.plot(range(10), results[:10], label='Armijo rn')\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"Function Value\")\n",
    "    plt.show()\n",
    "    return results\n",
    "\n",
    "\n",
    "def armijo_r2(f=None, dfx1=None, dfx2=None, t=1, count=1, x0=None, alpha=0.3, beta=0.8):\n",
    "    f = f if f is not None else lambda x: ((x[0] - 1) ** 2 + (x[1] - 4) ** 2)\n",
    "    dfx1 = dfx1 if dfx1 is not None else lambda x: (2 * x[0])\n",
    "    dfx2 = dfx2 if dfx2 is not None else lambda x: (2 * x[1])\n",
    "    x0 = x0 if x0 is not None else np.array([2, 3])\n",
    "\n",
    "    def backtrack(x0, dfx1, dfx2, t, alpha, beta, count):\n",
    "        fun_value = []\n",
    "        while (f(x0) - (f(x0 - t * np.array([dfx1(x0), dfx2(x0)])) + alpha * t * np.dot(np.array([dfx1(x0), dfx2(x0)]),\n",
    "                                                                                        np.array([dfx1(x0),\n",
    "                                                                                                  dfx2(x0)])))) < 0:\n",
    "            t *= beta\n",
    "            fun_value.append(t)\n",
    "            count += 1\n",
    "        return t, count, fun_value\n",
    "\n",
    "    t, count, fun_value = backtrack(x0, dfx1, dfx2, t, alpha, beta, count)\n",
    "    plt.plot(range(len(fun_value)), fun_value, label='Armijo r2')\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"Function Value\")\n",
    "    plt.show()\n",
    "#    print(\"\\nfinal step size :\", t, \" \\nnombre d'iteration Armijo: \", count)\n",
    "    print(\"\\nfinal step size :\", t, \" \\nnombre d'iteration Armijo: \", count)\n",
    "\n",
    "#    return fun_value\n",
    "    return None\n",
    "\n",
    "def user_input():\n",
    "    Aa = int(input(\"Enter the number of rows for A (ex: 2):\"))\n",
    "    Ab = int(input(\"Enter the number of columns for A(ex: 2):\"))\n",
    "\n",
    "    print(\"Enter the entries in a single line for A (separated by space ex: 1 2 3 4): \")\n",
    "\n",
    "    entries = list(map(int, input().split()))\n",
    "    matrixA = np.matrix(entries).reshape(Aa, Ab)\n",
    "    print(type(matrixA))\n",
    "\n",
    "    b = int(input(\"Enter the number of rows for b (ex: 5):\"))\n",
    "    bcolumns = 1\n",
    "    entries = list(map(int, input().split()))\n",
    "    matrixb = np.matrix(entries).reshape(b, bcolumns)\n",
    "    armijo(A=matrixA, b=matrixb)\n",
    "\n",
    "# c1 = time()\n",
    "# print(\"armijo r2 :\", armijo_r2())\n",
    "# print('temps de execution Armijo r2 : ', time() - c1)\n",
    "\n",
    "# c = time()\n",
    "# print(\"armijo rn :\", armijo())\n",
    "# print('temps de execution Armijo : ', time() - c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b5c34674",
   "metadata": {},
   "outputs": [],
   "source": [
    "#group 8\n",
    "from scipy.optimize import minimize,LinearConstraint,Bounds\n",
    "# import numpy as np\n",
    "import numdifftools as nd\n",
    "\n",
    "from scipy.optimize import linprog\n",
    "import time\n",
    "\n",
    "def min_fun(x):\n",
    "    return (x[0]-1)**2 + (x[1]-4)**2\n",
    "\n",
    "\n",
    "class frank_wolfe():\n",
    "\n",
    "    def __init__(self, min_fun, A, b, bounds, x0, iterations=100):\n",
    "        self.min_fun = min_fun\n",
    "        self.A = A\n",
    "        self.b = b\n",
    "        self.bounds = bounds\n",
    "        self.x0 = x0\n",
    "        self.iterations = iterations\n",
    "        self.x_min = []\n",
    "        self.f_min = 0\n",
    "        self.x_t = []\n",
    "        self.s_t = []\n",
    "        self.f_t = []\n",
    "        self.violation = 0\n",
    "        self.time = 0\n",
    "\n",
    "    def __repr__(self):\n",
    "        out = 'f_min: ' + str(self.f_min) + '\\n' + \\\n",
    "              'x_min: ' + str(self.x_min) + '\\n' + \\\n",
    "              'violation: ' + str(self.violation) + '\\n' + \\\n",
    "              'time: ' + str(self.time)\n",
    "        return out\n",
    "\n",
    "    def optimize(self):\n",
    "        x = self.x0\n",
    "        t1 = time.time()\n",
    "        for i in range(0, self.iterations):\n",
    "            gamma = 2 / (i + 2)\n",
    "            grad_def = nd.Gradient(self.min_fun)\n",
    "            grad = grad_def(x)\n",
    "            update = linprog(grad, A_ub=self.A, b_ub=self.b, A_eq=None, b_eq=None, bounds=self.bounds,\n",
    "                             method='interior-point', callback=None, x0=None,\n",
    "                             options={'sym_pos': False, 'lstsq': True})\n",
    "\n",
    "            s = update.x\n",
    "            self.s_t.append(s)\n",
    "            x = x + gamma * (s - x)\n",
    "            self.f_t.append(self.min_fun(x))\n",
    "            self.x_t.append(x)\n",
    "        t2 = time.time()\n",
    "        self.time = t2 - t1\n",
    "\n",
    "        constraints = np.dot(self.A, x) - self.b\n",
    "        self.violation = np.sum([i for i in constraints if i > 0])\n",
    "        self.x_min = x\n",
    "        self.f_min = self.f_t[-1]\n",
    "        return self\n",
    "\n",
    "def fonction8():\n",
    "    constr_num = 2\n",
    "    var_num = 3\n",
    "\n",
    "    A = np.random.randint(-10,10, (constr_num,var_num))\n",
    "    ub = 10*np.ones(constr_num)\n",
    "\n",
    "    bounds=[(0,10) for i in range(0,var_num)]\n",
    "    x0 = np.random.randint(0,10,(var_num))\n",
    "\n",
    "\n",
    "    iterations = 200\n",
    "    fw = frank_wolfe(min_fun,A,ub,bounds,x0,iterations)\n",
    "    results = fw.optimize()\n",
    "\n",
    "\n",
    "    bounds = Bounds(np.zeros(var_num), 10*np.ones(var_num))\n",
    "    lb = -np.inf*np.ones(constr_num)\n",
    "    linear_constraint = LinearConstraint(A, lb, ub)\n",
    "\n",
    "    res = minimize(min_fun, x0, method='trust-constr', jac=nd.Gradient(min_fun),\n",
    "                    constraints=linear_constraint, bounds=bounds,\n",
    "                    options={'verbose': 0,'gtol': 1e-8, 'disp': True})\n",
    "\n",
    "    f_star = [res.fun for i in range(0,len(results.f_t))]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "266d20ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#group 9\n",
    "\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plot\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from itertools import product\n",
    "from warnings import warn\n",
    "from sklearn.datasets import make_spd_matrix\n",
    "\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "\n",
    "def WolfeLineSearch(f, f_grad, xk, pk, c1=1e-4, c2=0.9, amax=None, maxiter=10):\n",
    "    \"\"\"\n",
    "    Find alpha that satisfies strong Wolfe conditions.\n",
    "    Parameters\n",
    "    ----------\n",
    "    f : callable f(x)\n",
    "        Objective function.\n",
    "    f_grad : callable f'(x)\n",
    "        Objective function gradient.\n",
    "    xk : ndarray\n",
    "        Starting point.\n",
    "    pk : ndarray\n",
    "        Search direction.\n",
    "    c1 : float, optional\n",
    "        Parameter for Armijo condition rule.\n",
    "    c2 : float, optional\n",
    "        Parameter for curvature condition rule.\n",
    "    amax : float, optional\n",
    "        Maximum step size\n",
    "    maxiter : int, optional\n",
    "        Maximum number of iterations to perform.\n",
    "    Returns\n",
    "    -------\n",
    "    alpha : float or None\n",
    "        Alpha for which ``x_new = x0 + alpha * pk``,\n",
    "        or None if the line search algorithm did not converge.\n",
    "    phi : float or None\n",
    "        New function value ``f(x_new)=f(x0+alpha*pk)``,\n",
    "        or None if the line search algorithm did not converge.\n",
    "    \"\"\"\n",
    "\n",
    "    def phi(alpha):\n",
    "        return f(xk + alpha * pk)\n",
    "\n",
    "    def derphi(alpha):\n",
    "        return np.dot(f_grad(xk + alpha * pk), pk)\n",
    "\n",
    "    alpha_star, phi_star, derphi_star = WolfeLineSearch2(phi, derphi, c1, c2, amax, maxiter)\n",
    "\n",
    "    if derphi_star is None:\n",
    "        warn('The line search algorithm did not converge', RuntimeWarning)\n",
    "\n",
    "    return alpha_star, phi_star\n",
    "\n",
    "\n",
    "def WolfeLineSearch2(phi, derphi, c1=1e-4, c2=0.9, amax=None, maxiter=10):\n",
    "    \"\"\"\n",
    "    Find alpha that satisfies strong Wolfe conditions.\n",
    "    alpha > 0 is assumed to be a descent direction.\n",
    "    Parameters\n",
    "    ----------\n",
    "    phi : callable phi(alpha)\n",
    "        Objective scalar function.\n",
    "    derphi : callable phi'(alpha)\n",
    "        Objective function derivative. Returns a scalar.\n",
    "    c1 : float, optional\n",
    "        Parameter for Armijo condition rule.\n",
    "    c2 : float, optional\n",
    "        Parameter for curvature condition rule.\n",
    "    amax : float, optional\n",
    "        Maximum step size.\n",
    "    maxiter : int, optional\n",
    "        Maximum number of iterations to perform.\n",
    "    Returns\n",
    "    -------\n",
    "    alpha_star : float or None\n",
    "        Best alpha, or None if the line search algorithm did not converge.\n",
    "    phi_star : float\n",
    "        phi at alpha_star.\n",
    "    derphi_star : float or None\n",
    "        derphi at alpha_star, or None if the line search algorithm\n",
    "        did not converge.\n",
    "    \"\"\"\n",
    "    \n",
    "    phi0 = phi(0.)\n",
    "    derphi0 = derphi(0.)\n",
    "\n",
    "    alpha0 = 0\n",
    "    alpha1 = 1.0\n",
    "\n",
    "    if amax is not None:\n",
    "        alpha1 = min(alpha1, amax)\n",
    "\n",
    "    phi_a1 = phi(alpha1)\n",
    "    #derphi_a1 = derphi(alpha1) evaluated below\n",
    "\n",
    "    phi_a0 = phi0\n",
    "    derphi_a0 = derphi0\n",
    "\n",
    "    for i in range(maxiter):\n",
    "        if alpha1 == 0 or (amax is not None and alpha0 == amax):\n",
    "            # alpha1 == 0: This shouldn't happen. Perhaps the increment has\n",
    "            # slipped below machine precision?\n",
    "            alpha_star = None\n",
    "            phi_star = phi0\n",
    "            derphi_star = None\n",
    "\n",
    "            if alpha1 == 0:\n",
    "                msg = 'Rounding errors prevent the line search from converging'\n",
    "            else:\n",
    "                msg = \"The line search algorithm could not find a solution \" +                       \"less than or equal to amax: %s\" % amax\n",
    "\n",
    "            warn(msg, RuntimeWarning)\n",
    "            break\n",
    "\n",
    "        if (phi_a1 > phi0 + c1 * alpha1 * derphi0) or            ((phi_a1 >= phi_a0) and (i > 1)):\n",
    "            alpha_star, phi_star, derphi_star =                         _zoom(alpha0, alpha1, phi_a0,\n",
    "                              phi_a1, derphi_a0, phi, derphi,\n",
    "                              phi0, derphi0, c1, c2)\n",
    "            break\n",
    "\n",
    "        derphi_a1 = derphi(alpha1)\n",
    "        if (abs(derphi_a1) <= -c2*derphi0):\n",
    "            alpha_star = alpha1\n",
    "            phi_star = phi_a1\n",
    "            derphi_star = derphi_a1\n",
    "            break\n",
    "\n",
    "        if (derphi_a1 >= 0):\n",
    "            alpha_star, phi_star, derphi_star =                         _zoom(alpha1, alpha0, phi_a1,\n",
    "                              phi_a0, derphi_a1, phi, derphi,\n",
    "                              phi0, derphi0, c1, c2)\n",
    "            break\n",
    "\n",
    "        alpha2 = 2 * alpha1  # increase by factor of two on each iteration\n",
    "        if amax is not None:\n",
    "            alpha2 = min(alpha2, amax)\n",
    "        alpha0 = alpha1\n",
    "        alpha1 = alpha2\n",
    "        phi_a0 = phi_a1\n",
    "        phi_a1 = phi(alpha1)\n",
    "        derphi_a0 = derphi_a1\n",
    "\n",
    "    else:\n",
    "        # stopping test maxiter reached\n",
    "        alpha_star = alpha1\n",
    "        phi_star = phi_a1\n",
    "        derphi_star = None\n",
    "        warn('The line search algorithm did not converge', RuntimeWarning)\n",
    "\n",
    "    return alpha_star, phi_star, derphi_star\n",
    "\n",
    "\n",
    "def _cubicmin(a, fa, fpa, b, fb, c, fc):\n",
    "    \"\"\"\n",
    "    Finds the minimizer for a cubic polynomial that goes through the\n",
    "    points (a,fa), (b,fb), and (c,fc) with derivative at a of fpa.\n",
    "    If no minimizer can be found, return None.\n",
    "    \"\"\"\n",
    "    # f(x) = A *(x-a)^3 + B*(x-a)^2 + C*(x-a) + D\n",
    "    with np.errstate(divide='raise', over='raise', invalid='raise'):\n",
    "        try:\n",
    "            C = fpa\n",
    "            db = b - a\n",
    "            dc = c - a\n",
    "            denom = (db * dc) ** 2 * (db - dc)\n",
    "            d1 = np.empty((2, 2))\n",
    "            d1[0, 0] = dc ** 2\n",
    "            d1[0, 1] = -db ** 2\n",
    "            d1[1, 0] = -dc ** 3\n",
    "            d1[1, 1] = db ** 3\n",
    "            [A, B] = np.dot(d1, np.asarray([fb - fa - C * db,\n",
    "                                            fc - fa - C * dc]).flatten())\n",
    "            A /= denom\n",
    "            B /= denom\n",
    "            radical = B * B - 3 * A * C\n",
    "            xmin = a + (-B + np.sqrt(radical)) / (3 * A)\n",
    "        except ArithmeticError:\n",
    "            return None\n",
    "    if not np.isfinite(xmin):\n",
    "        return None\n",
    "    return xmin\n",
    "\n",
    "\n",
    "def _quadmin(a, fa, fpa, b, fb):\n",
    "    \"\"\"\n",
    "    Finds the minimizer for a quadratic polynomial that goes through\n",
    "    the points (a,fa), (b,fb) with derivative at a of fpa.\n",
    "    \"\"\"\n",
    "    # f(x) = B*(x-a)^2 + C*(x-a) + D\n",
    "    with np.errstate(divide='raise', over='raise', invalid='raise'):\n",
    "        try:\n",
    "            D = fa\n",
    "            C = fpa\n",
    "            db = b - a * 1.0\n",
    "            B = (fb - D - C * db) / (db * db)\n",
    "            xmin = a - C / (2.0 * B)\n",
    "        except ArithmeticError:\n",
    "            return None\n",
    "    if not np.isfinite(xmin):\n",
    "        return None\n",
    "    return xmin\n",
    "\n",
    "\n",
    "def _zoom(a_lo, a_hi, phi_lo, phi_hi, derphi_lo,\n",
    "          phi, derphi, phi0, derphi0, c1, c2):\n",
    "    \"\"\"\n",
    "    Zoom stage of approximate linesearch satisfying strong Wolfe conditions.\n",
    "    \"\"\"\n",
    "\n",
    "    maxiter = 10\n",
    "    i = 0\n",
    "    delta1 = 0.2  # cubic interpolant check\n",
    "    delta2 = 0.1  # quadratic interpolant check\n",
    "    phi_rec = phi0\n",
    "    a_rec = 0\n",
    "    while True:\n",
    "        # interpolate to find a trial step length between a_lo and\n",
    "        # a_hi Need to choose interpolation here. Use cubic\n",
    "        # interpolation and then if the result is within delta *\n",
    "        # dalpha or outside of the interval bounded by a_lo or a_hi\n",
    "        # then use quadratic interpolation, if the result is still too\n",
    "        # close, then use bisection\n",
    "\n",
    "        dalpha = a_hi - a_lo\n",
    "        if dalpha < 0:\n",
    "            a, b = a_hi, a_lo\n",
    "        else:\n",
    "            a, b = a_lo, a_hi\n",
    "\n",
    "        # minimizer of cubic interpolant\n",
    "        # (uses phi_lo, derphi_lo, phi_hi, and the most recent value of phi)\n",
    "        #\n",
    "        # if the result is too close to the end points (or out of the\n",
    "        # interval), then use quadratic interpolation with phi_lo,\n",
    "        # derphi_lo and phi_hi if the result is still too close to the\n",
    "        # end points (or out of the interval) then use bisection\n",
    "\n",
    "        if (i > 0):\n",
    "            cchk = delta1 * dalpha\n",
    "            a_j = _cubicmin(a_lo, phi_lo, derphi_lo, a_hi, phi_hi,\n",
    "                            a_rec, phi_rec)\n",
    "        if (i == 0) or (a_j is None) or (a_j > b - cchk) or (a_j < a + cchk):\n",
    "            qchk = delta2 * dalpha\n",
    "            a_j = _quadmin(a_lo, phi_lo, derphi_lo, a_hi, phi_hi)\n",
    "            if (a_j is None) or (a_j > b-qchk) or (a_j < a+qchk):\n",
    "                a_j = a_lo + 0.5*dalpha\n",
    "\n",
    "        # Check new value of a_j\n",
    "\n",
    "        phi_aj = phi(a_j)\n",
    "        if (phi_aj > phi0 + c1*a_j*derphi0) or (phi_aj >= phi_lo):\n",
    "            phi_rec = phi_hi\n",
    "            a_rec = a_hi\n",
    "            a_hi = a_j\n",
    "            phi_hi = phi_aj\n",
    "        else:\n",
    "            derphi_aj = derphi(a_j)\n",
    "            if abs(derphi_aj) <= -c2*derphi0:\n",
    "                a_star = a_j\n",
    "                val_star = phi_aj\n",
    "                valprime_star = derphi_aj\n",
    "                break\n",
    "            if derphi_aj*(a_hi - a_lo) >= 0:\n",
    "                phi_rec = phi_hi\n",
    "                a_rec = a_hi\n",
    "                a_hi = a_lo\n",
    "                phi_hi = phi_lo\n",
    "            else:\n",
    "                phi_rec = phi_lo\n",
    "                a_rec = a_lo\n",
    "            a_lo = a_j\n",
    "            phi_lo = phi_aj\n",
    "            derphi_lo = derphi_aj\n",
    "        i += 1\n",
    "        if (i > maxiter):\n",
    "            # Failed to find a conforming step size\n",
    "            a_star = None\n",
    "            val_star = None\n",
    "            valprime_star = None\n",
    "            break\n",
    "    return a_star, val_star, valprime_star\n",
    "\n",
    "\n",
    "# In[10]:\n",
    "\n",
    "\n",
    "def Griewank(xs):\n",
    "    \"\"\"Griewank Function\"\"\"\n",
    "    d = len(xs)\n",
    "    sqrts = np.array([np.sqrt(i + 1) for i in range(d)])\n",
    "    cos_terms = np.cos(xs / sqrts)\n",
    "    \n",
    "    sigma = np.dot(xs, xs) / 4000\n",
    "    pi = np.prod(cos_terms)\n",
    "    return 1 + sigma - pi\n",
    "\n",
    "def GriewankGrad(xs):\n",
    "    \"\"\"First derivative of Griewank Function\"\"\"\n",
    "    d = len(xs)\n",
    "    sqrts = np.array([np.sqrt(i + 1) for i in range(d)])\n",
    "    cos_terms = np.cos(xs / sqrts)\n",
    "    pi_coefs = np.prod(cos_terms) / cos_terms\n",
    "    \n",
    "    sigma = 2 * xs / 4000\n",
    "    pi = pi_coefs * np.sin(xs / sqrts) * (1 / sqrts)\n",
    "    return sigma + pi\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[11]:\n",
    "\n",
    "\n",
    "def NonlinearCG(f, f_grad, init, method='FR', c1=1e-4, c2=0.1, amax=None, tol=1e-5, max_iter=1000):\n",
    "    \"\"\"Non Linear Conjugate Gradient Method for optimization problem.\n",
    "    Given a starting point x ∈ ℝⁿ.\n",
    "    repeat\n",
    "        1. Calculate step length alpha using Wolfe Line Search.\n",
    "        2. Update x_new = x + alpha * p.\n",
    "        3. Calculate beta using one of available methods.\n",
    "        4. Update p = -f_grad(x_new) + beta * p\n",
    "    until stopping criterion is satisfied.\n",
    "    \n",
    "    Parameters\n",
    "    --------------------\n",
    "        f        : function to optimize\n",
    "        f_grad   : first derivative of f\n",
    "        init     : initial value of x, can be set to be any numpy vector,\n",
    "        method   : method to calculate beta, can be one of the followings: FR, PR, HS, DY, HZ.\n",
    "        c1       : Armijo constant\n",
    "        c2       : Wolfe constant\n",
    "        amax     : maximum step size\n",
    "        tol      : tolerance of the difference of the gradient norm to zero\n",
    "        max_iter : maximum number of iterations\n",
    "        \n",
    "    Returns\n",
    "    --------------------\n",
    "        curve_x  : x in the learning path\n",
    "        curve_y  : f(x) in the learning path\n",
    "    \"\"\"\n",
    "    \n",
    "    # initialize some values\n",
    "    x = init\n",
    "    y = f(x)\n",
    "    gfk = f_grad(x)\n",
    "    p = -gfk\n",
    "    gfk_norm = np.linalg.norm(gfk)\n",
    "    \n",
    "    # for result tabulation\n",
    "    num_iter = 0\n",
    "    curve_x = [x]\n",
    "    curve_y = [y]\n",
    "    print('Initial condition: y = {:.4f}, x = {} \\n'.format(y, x))\n",
    "    \n",
    "    # begin iteration\n",
    "    while gfk_norm > tol and num_iter < max_iter:\n",
    "        # search for step size alpha\n",
    "        alpha, y_new = WolfeLineSearch(f, f_grad, x, p, c1=c1, c2=c2, amax=amax)\n",
    "        \n",
    "        # update iterate x\n",
    "        x_new = x + alpha * p\n",
    "        gf_new = f_grad(x_new)\n",
    "        \n",
    "        # calculate beta\n",
    "        if method == 'FR':\n",
    "            beta = np.dot(gf_new, gf_new) / np.dot(gfk, gfk)\n",
    "        elif method == 'PR':\n",
    "            y_hat = gf_new - gfk\n",
    "            beta = np.dot(gf_new, y_hat) / np.dot(gfk, gfk)\n",
    "        elif method == 'HS':\n",
    "            y_hat = gf_new - gfk\n",
    "            beta = np.dot(y_hat, gf_new) / np.dot(y_hat, p)\n",
    "        elif method == 'DY':\n",
    "            y_hat = gf_new - gfk\n",
    "            beta = np.dot(gf_new, gf_new) / np.dot(y_hat, p)\n",
    "        elif method == 'HZ':\n",
    "            y_hat = gf_new - gfk\n",
    "            beta = np.dot(y_hat, gf_new) / np.dot(y_hat, p)\n",
    "            beta = beta - 2 * np.dot(y_hat, y_hat) * np.dot(p, gf_new) / (np.dot(y_hat, p) ** 2)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                'Method is unrecognizable. Try one of the following values: FR, PR, HS, DY, HZ.'\n",
    "            )\n",
    "        \n",
    "        # update everything\n",
    "        error = y - y_new\n",
    "        x = x_new\n",
    "        y = y_new\n",
    "        gfk = gf_new\n",
    "        p = -gfk + beta * p\n",
    "        gfk_norm = np.linalg.norm(gfk)\n",
    "        \n",
    "        # result tabulation\n",
    "        num_iter += 1\n",
    "        curve_x.append(x)\n",
    "        curve_y.append(y)\n",
    "        print('Iteration: {} \\t y = {:.4f}, x = {}, gradient = {:.4f}'.\n",
    "              format(num_iter, y, x, gfk_norm))\n",
    "    \n",
    "    # print results\n",
    "    if num_iter == max_iter:\n",
    "        print('\\nGradient descent does not converge.')\n",
    "    else:\n",
    "        print('\\nSolution: \\t y = {:.4f}, x = {}'.format(y, x))\n",
    "    \n",
    "    return np.array(curve_x), np.array(curve_y)\n",
    "\n",
    "\n",
    "# In[12]:\n",
    "\n",
    "\n",
    "def create_mesh(Griewank):\n",
    "    x = np.arange(-5, 5, 0.025)\n",
    "    y = np.arange(-5, 5, 0.025)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    Z = np.zeros(X.shape)\n",
    "    mesh_size = range(len(X))\n",
    "    for i, j in product(mesh_size, mesh_size):\n",
    "        x_coor = X[i][j]\n",
    "        y_coor = Y[i][j]\n",
    "        Z[i][j] = Griewank(np.array([x_coor, y_coor]))\n",
    "    return X, Y, Z\n",
    "\n",
    "def plot_contour(ax, X, Y, Z):\n",
    "    ax.set(\n",
    "        title='Path During Optimization Process',\n",
    "        xlabel='x1',\n",
    "        ylabel='x2'\n",
    "    )\n",
    "    CS = ax.contour(X, Y, Z)\n",
    "    ax.clabel(CS, fontsize='smaller', fmt='%1.2f')\n",
    "    ax.axis('square')\n",
    "    return ax\n",
    "    \n",
    "\n",
    "def plot_value(ax):\n",
    "    ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    ax.set(\n",
    "        title='Objective Function Value During Optimization Process',\n",
    "        xlabel='Iterations',\n",
    "        ylabel='Objective Function Value'\n",
    "    )\n",
    "    ax.legend(['Wolfe line search algorithm'])\n",
    "    return ax\n",
    "\n",
    "\n",
    "def plot(xs, ys):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    plt.suptitle('Gradient Descent Method')\n",
    "    \n",
    "    ax1 = plot_contour(ax1, X, Y, Z)\n",
    "    ax1.plot(xs[:,0], xs[:,1], linestyle='--', marker='o', color='orange')\n",
    "    ax1.plot(xs[-1,0], xs[-1,1], 'ro')\n",
    "    \n",
    "    ax2 = plot_value(ax2)\n",
    "    ax2.plot(ys, linestyle='--', marker='o', color='orange')\n",
    "    ax2.plot(len(ys)-1, ys[-1], 'ro')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# In[14]:\n",
    "\n",
    "def fonction9():\n",
    "    fig, ax= plt.subplots(figsize=(6, 6))\n",
    "    X, Y, Z = create_mesh(Griewank)\n",
    "    ax = plot_contour(ax, X, Y, Z)\n",
    "    #ax.plot(xs[:,0], xs[:,1], linestyle='--', marker='o', color='orange')\n",
    "    #ax.plot(xs[-1,0], xs[-1,1], 'ro')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # In[13]:\n",
    "\n",
    "\n",
    "    X, Y, Z = create_mesh(Griewank)\n",
    "    \n",
    "    x0 = np.array([2, 1])\n",
    "    xs, ys = NonlinearCG(Griewank, GriewankGrad, init=x0, method='PR')\n",
    "#     plot(xs, ys)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4224e232",
   "metadata": {},
   "outputs": [],
   "source": [
    "from termcolor import colored\n",
    "def mainf():    \n",
    "    if (menuu.value == '---'):\n",
    "        print(\"choisir fonction\")\n",
    "    elif (menuu.value == 'fct1'):\n",
    "        print(colored(\"Graph\",\"blue\"))\n",
    "        GrapheLN(fct1,-1,1,-1,2,20,10)\n",
    "        print(colored(\"Vecteur Gradient et Matrice hessienne\",\"blue\"))\n",
    "        fonction2(1)\n",
    "        print(colored(\"fonction pour la méthode de gradient à pas décroissant \",\"blue\"))\n",
    "        fonction4(1)\n",
    "        print(colored(\"fonction pour le calcul d'Armijo  \",\"blue\"))\n",
    "        armijo_r2()\n",
    "        print(colored(\"fonction pour le calcul de Wolfe   \",\"blue\"))\n",
    "        fonction8()\n",
    "\n",
    "    elif (menuu.value == 'fct2'):\n",
    "        print(\"Graph\")\n",
    "        GrapheLN(fct2,-1,1,-1,2,20,10)\n",
    "        print(colored(\"Vecteur Gradient et Matrice hessienne\",'blue'))\n",
    "        fonction2(2)\n",
    "        print(colored(\"fonction pour la méthode de gradient à pas fixe \",\"blue\"))\n",
    "        fonction3(2)\n",
    "        print(colored(\"fonction pour la méthode de gradient à pas décroissant \",\"blue\"))\n",
    "        fonction4(2)\n",
    "        print(colored(\"fonction pour la méthode de gradient à pas optimal  \",\"blue\"))\n",
    "        fonction5(2,0)\n",
    "\n",
    "\n",
    "    elif (menuu.value == 'fct3'):\n",
    "        print(colored(\"Graph\",\"blue\"))\n",
    "        GrapheLN(fct3,-1,1,-1,2,20,10)\n",
    "        print(colored(\"Vecteur Gradient et Matrice hessienne\",\"blue\"))\n",
    "    #     fonction2(3)\n",
    "        print(colored(\"fonction pour la méthode de gradient à pas fixe \",\"blue\"))\n",
    "        fonction3(3)\n",
    "        print(colored(\"fonction pour la méthode de gradient à pas décroissant \",\"blue\"))\n",
    "        fonction4(3)\n",
    "        print(colored(\"fonction pour la méthode de gradient à pas optimal  \",\"blue\"))\n",
    "        fonction5(3,0)\n",
    "    elif (menuu.value == 'fct4'):\n",
    "        print(colored(\"fonction pour la méthode de gradient conjugué standard   \",\"blue\"))\n",
    "        fonction6()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e4fc809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some handy functions to use along widgets\n",
    "from IPython.display import display, Markdown, clear_output\n",
    "# widget packages\n",
    "import ipywidgets as widgets\n",
    "# defining some widgets\n",
    "text = widgets.Text(\n",
    "       value='My Text',\n",
    "       description='Title', )\n",
    "menu = widgets.Dropdown(\n",
    "       options=['---', 'saisir fonction', 'fonction de test'],\n",
    "       value='---',\n",
    "       description='choix:')\n",
    "checkbox = widgets.Checkbox(\n",
    "           description='Check to invert',)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cde252e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c12e5f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def menuf():\n",
    "    menuu=None\n",
    "    if (menu.value == 'fonction de test'):\n",
    "        fct1=eval('lambda x,y: (x-1)**2+(y-4)**2')\n",
    "        fct2=eval('lambda x,y: x**2+y**2')\n",
    "        fct3=eval('lambda x,y: (1-x)**2+100*(y-x**2)**2')\n",
    "        menuu = widgets.Dropdown(\n",
    "               options=['---', 'fct1', 'fct2' , 'fct3' ,'fct4'],\n",
    "               value='---',\n",
    "               description='choix test:')\n",
    "    display(menuu)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "95e5762f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'value'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_630/3670390320.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmainf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_630/2005382464.py\u001b[0m in \u001b[0;36mmainf\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtermcolor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcolored\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmainf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmenuu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'---'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"choisir fonction\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmenuu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'fct1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'value'"
     ]
    }
   ],
   "source": [
    "mainf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "55d57bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def menus():\n",
    "    menuuu=None\n",
    "    if (menu.value == 'saisir fonction'):\n",
    "        print(\"[1]: fonction pour le graphe et les lignes de niveaux de n'importe quelle fonction \")\n",
    "        print(\"[4]: fonction pour la méthode de gradient à pas décroissant \")\n",
    "        print(\"[5]: fonction pour la méthode de gradient à pas optimal  \")\n",
    "        print(\"[6]: fonction pour la méthode de gradient conjugué standard   \")\n",
    "        print(\"[7]: fonction pour le calcul d'Armijo    \")\n",
    "        menuuu = widgets.Dropdown(\n",
    "               options=['---','1', '4', '5' , '6' ,'7'],\n",
    "               value='---',\n",
    "               description='choix:')\n",
    "        display(menuuu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ea01305a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def mains():\n",
    "    if (menu.value == 'saisir fonction'):\n",
    "        if (menuuu.value == '1'):\n",
    "            GrapheLN(None,-1,1,-1,2,20,0)\n",
    "            f=eval('lambda x,y: ' + input(\"Entrer fonction:\"))\n",
    "            GrapheLN(f,-1,1,-1,2,20,1)\n",
    "        if (menuuu.value == '4'):\n",
    "            sym = []\n",
    "            nbVar = input('Nombre de variables: ')\n",
    "            for i in range(int(nbVar)):\n",
    "              var = input(\"Entrez la variable numéro \" + str(i+1) + \": \")\n",
    "              sym.append(Symbol(var))\n",
    "            function= sympify(input(\"Entrez la fonction: \"))\n",
    "            gradient_fct2 = GradientDescent_RN(function=function,variables=sym)\n",
    "            gradient_fct2.start_gradient()\n",
    "        if (menuuu.value == '5'):\n",
    "            nbbb=int(input(\"Nombre de variables: \"))\n",
    "            fonction5(4,nbbb)\n",
    "        if (menuuu.value == '6'):\n",
    "            fonction6()\n",
    "        if (menuuu.value == '7'):\n",
    "            user_input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "249e007e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f761143dba494da2b2c451cb851c6bdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='choix:', index=1, options=('---', 'saisir fonction', 'fonction de test'), value='saisir …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "menu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7d383a01",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'value'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_630/640207271.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmenuu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'value'"
     ]
    }
   ],
   "source": [
    "print(menuu.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "25038c28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce98981c5a944ce1a1e9e56a8f1ee512",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='choix test:', options=('---', 'fct1', 'fct2', 'fct3', 'fct4'), value='---')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "menuf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ed805d46",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'value'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_630/3670390320.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmainf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_630/2005382464.py\u001b[0m in \u001b[0;36mmainf\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtermcolor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcolored\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmainf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmenuu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'---'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"choisir fonction\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmenuu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'fct1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'value'"
     ]
    }
   ],
   "source": [
    "mainf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1fbb7772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]: fonction pour le graphe et les lignes de niveaux de n'importe quelle fonction \n",
      "[4]: fonction pour la méthode de gradient à pas décroissant \n",
      "[5]: fonction pour la méthode de gradient à pas optimal  \n",
      "[6]: fonction pour la méthode de gradient conjugué standard   \n",
      "[7]: fonction pour le calcul d'Armijo    \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5c181291d424c2facf81d492aadd5ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='choix:', options=('---', '1', '4', '5', '6', '7'), value='---')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'---'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "menus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0d22b1a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f6cb39d1",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'value'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_630/3963003475.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'value'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb4c33c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
